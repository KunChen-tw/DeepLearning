{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC8aEbYBZwRlVNb0BCBrd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunChen-tw/DeepLearning/blob/main/Practice_8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice 8-1"
      ],
      "metadata": {
        "id": "2sOP17Wvk2ze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC1aQFo2j_40"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# 定義對應的頻率和標籤\n",
        "frequencies = [261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25]\n",
        "\n",
        "# Generate simple sine wave audio\n",
        "def generate_noisy_sine_wave(freq, sample_rate=16000, duration=1, noise_level=0.05):\n",
        "    t = torch.linspace(0, duration, int(sample_rate * duration), dtype=torch.float32)\n",
        "    waveform = 0.5 * torch.sin(2 * torch.pi * freq * t)\n",
        "    noise = noise_level * torch.randn_like(waveform)\n",
        "    return waveform + noise\n",
        "\n",
        "# Define dataset class\n",
        "class SimpleAudioDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for _ in range(50):\n",
        "            for label, freq in enumerate(frequencies):\n",
        "                sine_wave = generate_noisy_sine_wave(freq)\n",
        "                self.data.append(sine_wave)\n",
        "                self.labels.append(label)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        waveform = self.data[index].unsqueeze(0)\n",
        "        label = self.labels[index]\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, label\n",
        "\n",
        "# Define audio transformation\n",
        "class AudioTransform:\n",
        "    def __init__(self):\n",
        "        self.mel_spectrogram = MelSpectrogram(\n",
        "            sample_rate=16000,\n",
        "            n_mels=32, # The numbers of filter in Mel  spectrum\n",
        "            n_fft=400, # The window of FFT\n",
        "            hop_length=160) # The stride(步長) between each FFT\n",
        "        self.amplitude_to_db = AmplitudeToDB()\n",
        "\n",
        "    def __call__(self, waveform):\n",
        "        mel_spec = self.mel_spectrogram(waveform)\n",
        "        mel_spec_db = self.amplitude_to_db(mel_spec)\n",
        "        return mel_spec_db\n",
        "\n",
        "transform = AudioTransform()\n",
        "dataset = SimpleAudioDataset(transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Initialize device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define CNN model\n",
        "class SimpleAudioModel(nn.Module):\n",
        "    def __init__(self, sample_input):\n",
        "        super(SimpleAudioModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1).to(device)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2).to(device)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1).to(device)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2).to(device)\n",
        "        self.flatten = nn.Flatten().to(device)\n",
        "\n",
        "        # Dynamically calculate flatten size using sample input\n",
        "        sample_input = sample_input.to(device)\n",
        "        self.flatten_size = self._get_flatten_size(sample_input)\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 64).to(device)\n",
        "        self.fc2 = nn.Linear(64, 8).to(device)\n",
        "\n",
        "    def _get_flatten_size(self, sample_input):\n",
        "          with torch.no_grad():\n",
        "              sample_input = sample_input.to(device)\n",
        "              x = self.pool1(torch.relu(self.conv1(sample_input)))\n",
        "              x = self.pool2(torch.relu(self.conv2(x)))\n",
        "              x = self.flatten(x)\n",
        "              return x.shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "sample_input, _ = next(iter(train_loader))\n",
        "sample_input = sample_input.to(device)\n",
        "model = SimpleAudioModel(sample_input).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_model(model, train_loader, criterion, optimizer)\n",
        "    accuracy = evaluate_model(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, waveform):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        waveform = transform(waveform.unsqueeze(0)).to(device)\n",
        "        output = model(waveform.unsqueeze(0))\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Function to play audio\n",
        "def play_audio(waveform, sample_rate=16000):\n",
        "    # Ensure audio is on CPU and convert to NumPy\n",
        "    waveform_np = waveform.numpy()\n",
        "    sd.play(waveform_np, sample_rate)\n",
        "    sd.wait()\n",
        "\n",
        "#%%\n",
        "\n",
        "# Test model prediction and audio playback\n",
        "for freq in frequencies:\n",
        "    test_waveform = generate_noisy_sine_wave(freq)\n",
        "    predicted_label = predict(model, test_waveform)\n",
        "    print(f\"Predicted label for {freq} Hz: {predicted_label}\")\n",
        "    play_audio(test_waveform)\n",
        "\n"
      ]
    }
  ]
}